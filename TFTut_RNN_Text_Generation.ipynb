{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOV3DPO/u4pZp3g5kca8wg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GlassesNoGlasses/TFProjects/blob/main/TFTut_RNN_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**:\n",
        "Given a text, have the model predict and output the next sequences of sentences.\n",
        "\n",
        "**Modal Goal**:\n",
        "Given a sequence of characters, predict the next character to come after.\n",
        "Reiterate the model to produce sentences, then a whole text stream."
      ],
      "metadata": {
        "id": "9XDdokEzzCrx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaENqFQTjceC"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "Zu6qSpGOn6MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5snShOSn5No",
        "outputId": "d01e1679-8e2f-484e-926b-8e01ecb845cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7JDJ5ltolFM",
        "outputId": "49bc1d78-612e-41fa-e9bf-7a54cda34c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vocab into a list, then each character is tokenized with a unique id.\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "dZ0YuREsomqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return characters based on their id representation defined above.\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "yR3UwML_pAAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join ids back into original stirngs\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "XH5bTmYJt_VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with id <-> char conversion:\n",
        "\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "# Split list of strings into characters.\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "print(chars)\n",
        "\n",
        "# Convert each char into a unique id\n",
        "ids = ids_from_chars(chars)\n",
        "print(ids)\n",
        "\n",
        "# Convert ids back into original chars\n",
        "reconvertedChars = chars_from_ids(ids)\n",
        "reconvertedChars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WAtLA1Lp99_",
        "outputId": "d6898b4d-4292-4f28-8ff9-a0833a209283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
            "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and assign character ids to all characters in original text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids\n",
        "\n",
        "# Convert ids into a stream of ids that represent the original text characters\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "VMWgGZ0Qqv8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence length of characters to train model on\n",
        "seq_length = 100"
      ],
      "metadata": {
        "id": "2HZ6N_YAvMj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential batches of size seq_length + 1\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "Am4F1E94vYVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3XTj7Mmviv-",
        "outputId": "78794264-8f93-4c6a-c7c4-d7fe6c0b44c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split input sequence into a data set of (input, label)\n",
        "# I.e. \"tensorflow\" = (\"tensorflo\", \"ensorflow\")\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "V2byPHkCy5ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data set based on our original sequence\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "M1kctUKIz2Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example training data\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "TUdQ57Sq0CN_",
        "outputId": "464b11b1-57c0-4853-a4be-d630a65b18b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test batches\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer to fit data into\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUDtRi5KH2TE",
        "outputId": "790dfc4a-7ae4-49c9-bb47-3eb318c213b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "Vh4bc_IMIbxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    # vocab_size: unique inputs + 1\n",
        "    # embedding_dim: output vector dimensions\n",
        "    # rnn_units: how many rnn used.\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    # log liklihood with vocab_size outputs\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "TM1lq_5dMI39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "# How it works\n",
        "'''\n",
        "For each batch of inputs, we take each character,\n",
        "map the character to an embedding layer, update the\n",
        "GRU model with the embedding layer of input character,\n",
        "take GRU output and updates the dense (output) value\n",
        "of the character appearing.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "INBpKxewMnbr",
        "outputId": "9cd16115-f5c1-4a3a-db4f-4d63dc3ed2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFor each batch of inputs, we take each character,\\nmap the character to an embedding layer, update the\\nGRU model with the embedding layer of input character,\\ntake GRU output and updates the dense (output) value\\nof the character appearing.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2EtSoLWNx1_",
        "outputId": "b61a74b7-708a-4328-e3c2-7becd97f0e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "DdUAnDBKN5K-",
        "outputId": "5e3c825b-cc9f-4aee-a531-5cefacc18f3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling distributions\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2hg4vfU0P26",
        "outputId": "9b7c84c8-2b14-4553-8493-adce4b900761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b', but, in foul mouth\\nAnd in the witness of his proper ear,\\nTo call him villain? and then to glance f'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"Rhi'n,ws'hyiRDIMjGSUdvU!Ek$cLPnBu?&VJ[UNK].AzkocfQkvTuBL.gUsDMmrmdQZxT[UNK],-X!P,Baqu!GJXvEjTY?LNMF\\nchUv.e,Y\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a loss function\n",
        "\n",
        "# from_logits=True because model is returning logits\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df4U5szJ0zRu",
        "outputId": "51a389e8-6f37-4d9c-98aa-f120c3fff781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1891747, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loss function\n",
        "\n",
        "'''\n",
        "Current loss should be similar to vocabulary size.\n",
        "The model is newly trained and should have a loss\n",
        "that is high.\n",
        "\n",
        "Higher loses => model is sure of wrong answers and\n",
        "badly initialized.\n",
        "'''\n",
        "\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz_uTDiR1LzC",
        "outputId": "d19967b2-5858-44d5-a5ae-c1ed36c18cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.96832"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration of model with optimizer and loss functions\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "HQNVID4D1uzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "pTbTy7tt19w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual Training process\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZjvjdqs2XEA",
        "outputId": "d70e59f6-47c7-4ed6-bb68-8f5301dca38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 55ms/step - loss: 2.7576\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 2.0087\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.7209\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.5542\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.4513\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3813\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3283\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2824\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2406\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2003\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1586\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1154\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0717\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0241\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9740\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9207\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8689\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8145\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7617\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Text Class\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "yTRL4Uxj5JwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Idea**: The training has been done. We want to track the model's prediction of the next character, as well as the model's internal state. In each character iteration, we pass in the new internal state of the model and its prediction. Run in a loop."
      ],
      "metadata": {
        "id": "AAPgONIH5YPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "X0z3iL_N5pZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VNsJ9Hm6B7y",
        "outputId": "3748703d-5c0f-4a60-8f1d-06fa545c5591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Go to; sir, your mistress were the sons?\n",
            "\n",
            "MIRANDA:\n",
            "Mistake me not.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "There was a matter; yet I know my heart\n",
            "And wished a charm of arm me left with them;\n",
            "For such a clouds are strength must entertain it.\n",
            "\n",
            "HERMIONE:\n",
            "Sound, sir, come.\n",
            "\n",
            "LUCIO:\n",
            "Sir, your resoluting wretch.\n",
            "\n",
            "Second Servant:\n",
            "He doth ever less high a life.\n",
            "\n",
            "ANGELO:\n",
            "Believe me, my good lord, I pray your speech, and learn\n",
            "A man that found our former plucks of liberty.\n",
            "Gow plays we most the end were all offenders' speak,\n",
            "That raised by the noble king my husband's fresh.\n",
            "And till she stood up unsaved:\n",
            "The tongues of the presence goes\n",
            "Uppressting on thy trifes,--\n",
            "Gentlemen, cocelon, Clifford, could not speak?\n",
            "Your son--his deeds since, we will continue\n",
            "then and weak such come back, but that to dream\n",
            "As to a feast of poher from his histed speed,\n",
            "Which thou hast pass to scrape, and live as stay.\n",
            "And, if would I were some penntrot\n",
            "Are sen a little under run,\n",
            "But faults from hence, milty unshort\n",
            "And leads the wedding c \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.438652992248535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvements:\n",
        "\n",
        "\n",
        "*   Train model longer (higher EPOCH value)\n",
        "*   Add another RNN layer for accuracy.\n",
        "*   Adjust temperature for more or less random predictions.\n",
        "* Generate **parallel** results, shown below, in the same time.\n",
        "\n"
      ],
      "metadata": {
        "id": "EqGUOi8g6gOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoJ9NR-37HAQ",
        "outputId": "f39fbab9-3b71-4017-a3a2-e7071a5abdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nSome out of sore lines:\\nGood dances can tell them and retaguing; and I chnocks my nation\\nVeruse degrees I will leave his welk at him,\\nNor with thy swelling heart\\nPardon Remaints: 'He help to say\\nI do retermine or to Bolingbriot's heart.\\n\\nADRAND:\\nFaith, my noble mother!' there is no more sorrow ensper is.\\n\\nLUCIO:\\nI take her. So use his passage.\\n\\nESCALUS:\\nMy lord.\\n\\nDUKE OF YORK:\\nWhat will you undertake the trumpets bold.\\n\\nHASTINGS:\\nI am the greatest deposide, that the slave\\nDeliver us will find a thousand grace.\\n\\nGLOUCESTER:\\nThey be in love?\\n\\nHASTINGS:\\nBreak thee to fly, Tapth, and wild marriage,\\n'Martared, nobless with our royal elders.\\n\\nALONSO:\\nI primose than my daughter's home, the drud I now\\nTo make an emperite things.\\n\\nKING RICHARD III:\\nThat shows you to make aing the steel?\\n\\nEXETER:\\nHere are the untimely face that figure is sicks:\\nI hear those gracious just princes where he abship\\nTo the deputy's deeds o'er one 'WESTER:\\n\\nPROSPERO:\\nBut they stand come now in my true.\\n\\nISABELLA:\\nTo \"\n",
            " b\"ROMEO:\\nI them as my thought to hear them, then I left to love thee\\nDoth knock me as a woy;\\nIf he had slip, for thou suck'd business.\\n\\nANGELO:\\nYou'll never speak o' the sun:\\nI'll bring you and for all my heart and downty his crown\\nBut for his soldier hand, the tribunes doth humbly\\nTo seek him from my heart; and thus I enford the\\nbuildren croke; take honour.\\n\\nANTIGONUS:\\nI thank thee forth; too good my faith you good\\nPare from the nuptial of his assuirt: a merchant\\nHave the children yet:\\nBy her own weak-bind too; the voices nobles highwer;\\nAnd when they would halve home\\nThat Lanours all your airy cannot:\\nRevolt their corrs, conceins us enter'd\\nThat you into mine enemies,\\nDeath, their frued to their heatted friends,\\nBring me to the dires, and they shall feed my love.\\nNow, as I live, by the benefit of such\\ndiscords and drown'd to the moon, not's;\\nThe mighty stuller is prone and lam,\\nThe Lord Anamie't.\\n\\nANd CORIOLANUS:\\nI'll be at days.\\n\\nVOLUMNIA:\\nI'fore, sir; and, to the bett.\\n\\nProvost:\\nHear me, I\"\n",
            " b\"ROMEO:\\nThe prince your son, with trees and you his worth and honour,\\nIn sofe mild beauteous princess, damn'd with me\\nThe pity come weeping it: there to my curse\\nAnd violent fires of his mine are.\\n\\nANGELO:\\nYe're line: thou shouldst this knave it?\\n\\nLEONTES:\\nDo, good prope,\\nIf you'll expect my commanded by now.\\n\\nDUKE VINCENTIO:\\nNorfolk, the duke with your ele. But what hast live,\\nWa' no more admers; and so apprehension;\\nNone but cheeks o' the soldiers; and it crueciless,\\nTill my Richard never tell him so.\\nWell, go to! for I can tell you all.\\n\\nTHOMAS Mury:\\nHis all your chief hath den, with not above he\\nOnce might strangled at with tread end.\\n\\nMARCIUS:\\nSir, ha?\\nWhere is the Earl of Wiltshire's heir,\\nAnd take a Juliet, give on my gage!\\n\\nKATHARINA:\\nNot so high sickle! thanks from heaven, commanded by\\nAs if you will not be full gox: they for the frums\\nAnd all thy words of receive she is experien;\\nI mind to marry Courteous part of sovereign\\nTit the determination of your son\\nWhich look into us all, an\"\n",
            " b\"ROMEO:\\nShe you the news? where is Lord Aumerlo;\\nAnd when I'll uncounter and banishment;\\nThe oxe fall's upon corns of my words,\\nMy mourning brew drops of such alone,\\nWill wash your canker'd justice, which hath return'd,\\nUnto a closeness and treason dreadful part of mine.\\n\\nKING HENRY VI:\\nArt thou suck'd with my choice and true\\nThan carry no less. The sullem not there to\\nbe any coriolanus.\\n\\nLord:\\nLook, what I profite,\\nLain as too like to aud, and some foures\\nthat found thy fortune or breathing instruments. My boy yourself\\nIn that reason should be thine on't.\\n\\nANTONIO:\\nThe business fright the honourable foe,\\nA sign of die what ever dark with yet\\nMurders to the death he would be whipped\\nI deep as kind account, and prayers\\nAnd follow to thine eyes, Edward all\\nRebeling a servicer, than stands repair.\\n\\nVerendy:\\nWe must think,--\\n\\nSICINIUS:\\nThese conclusion shall I could till he wail!\\n\\nKATHARINA:\\nHere's no great father. Most sit with impermed withal\\nThan we bid the air, they are rid a mother,\\nAnd foll\"\n",
            " b\"ROMEO:\\nHaving done, I pray you, sir: come. Leop yours.\\nNow if that death is his own part, here stands\\nThat you refled thee from Palians:\\nI mean they were a scoldet mouths to them.\\n\\nGRUMIO:\\nKept thou not, Juliet? if For thy hand, and\\nhis wounds be so my rapier's power:\\nMy lord, the king is no trickn some glint,\\nWas it off you and yours, as thou art take\\nYour highness' person meet and do eye our\\ndear and the third bow the sea much deadly head\\nBut from my brother's fault am honest will:\\nHave any grief that does be won.\\n\\nJULIET:\\nGo, tell the parlous go.\\n\\nRATCLIFF:\\nRuseab your fresh and to murder:\\nI must forget myself; and she is something impress'd,\\nTwo cast upon the chase, I'll bring my death\\nAnd that a curse shunn'd it:'s in heaven,\\nIf the devil, and now as to enforce his words.\\n\\nEXTON:\\nAs full of passing first for love.\\nNow more his father garland to my soul,\\nWhiles with a very defe thrusts till now?\\nCome, grace; hear thee come.\\nWhile, love, that we will lose you, then come from\\nFretchers; ta\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.529752492904663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/Export generator and models\n",
        "\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EdcgCsS7Yjk",
        "outputId": "ed344150-2c7c-4838-a35f-58726b350ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7d9cfc21ea10>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced/Custom training\n",
        "\n",
        "'''\n",
        "The main idea is to provide feedback to the model,\n",
        "for both positive and negative predicitons.\n",
        "We use grandients as a way to calculate the\n",
        "accuracy of the prediction value, then use an\n",
        "optimizer to update the model.\n",
        "'''\n",
        "\n",
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "vj9-Yq1-8IMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdlqLJ0Y8sb2",
        "outputId": "478a2e54-1d7a-4057-f105-54ff3b0d4e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 15s 58ms/step - loss: 2.7349\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d9c8a01cdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFNuFmsY88F1",
        "outputId": "f66118e7-3f61-426e-a12c-c675b2f146ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1733\n",
            "Epoch 1 Batch 50 Loss 2.0425\n",
            "Epoch 1 Batch 100 Loss 1.9666\n",
            "Epoch 1 Batch 150 Loss 1.8401\n",
            "\n",
            "Epoch 1 Loss: 2.0005\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8547\n",
            "Epoch 2 Batch 50 Loss 1.7791\n",
            "Epoch 2 Batch 100 Loss 1.7215\n",
            "Epoch 2 Batch 150 Loss 1.6276\n",
            "\n",
            "Epoch 2 Loss: 1.7205\n",
            "Time taken for 1 epoch 11.24 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6110\n",
            "Epoch 3 Batch 50 Loss 1.5703\n",
            "Epoch 3 Batch 100 Loss 1.5058\n",
            "Epoch 3 Batch 150 Loss 1.5066\n",
            "\n",
            "Epoch 3 Loss: 1.5574\n",
            "Time taken for 1 epoch 11.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4907\n",
            "Epoch 4 Batch 50 Loss 1.4788\n",
            "Epoch 4 Batch 100 Loss 1.4681\n",
            "Epoch 4 Batch 150 Loss 1.4504\n",
            "\n",
            "Epoch 4 Loss: 1.4579\n",
            "Time taken for 1 epoch 11.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3815\n",
            "Epoch 5 Batch 50 Loss 1.4295\n",
            "Epoch 5 Batch 100 Loss 1.4026\n",
            "Epoch 5 Batch 150 Loss 1.3364\n",
            "\n",
            "Epoch 5 Loss: 1.3882\n",
            "Time taken for 1 epoch 11.62 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3107\n",
            "Epoch 6 Batch 50 Loss 1.3291\n",
            "Epoch 6 Batch 100 Loss 1.3479\n",
            "Epoch 6 Batch 150 Loss 1.2800\n",
            "\n",
            "Epoch 6 Loss: 1.3345\n",
            "Time taken for 1 epoch 11.58 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2744\n",
            "Epoch 7 Batch 50 Loss 1.2958\n",
            "Epoch 7 Batch 100 Loss 1.3050\n",
            "Epoch 7 Batch 150 Loss 1.2901\n",
            "\n",
            "Epoch 7 Loss: 1.2907\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2155\n",
            "Epoch 8 Batch 50 Loss 1.2326\n",
            "Epoch 8 Batch 100 Loss 1.2405\n",
            "Epoch 8 Batch 150 Loss 1.2574\n",
            "\n",
            "Epoch 8 Loss: 1.2498\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1699\n",
            "Epoch 9 Batch 50 Loss 1.2187\n",
            "Epoch 9 Batch 100 Loss 1.2011\n",
            "Epoch 9 Batch 150 Loss 1.2529\n",
            "\n",
            "Epoch 9 Loss: 1.2119\n",
            "Time taken for 1 epoch 11.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1785\n",
            "Epoch 10 Batch 50 Loss 1.1714\n",
            "Epoch 10 Batch 100 Loss 1.1707\n",
            "Epoch 10 Batch 150 Loss 1.1561\n",
            "\n",
            "Epoch 10 Loss: 1.1722\n",
            "Time taken for 1 epoch 11.46 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states =  one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TomqXlNr9lDy",
        "outputId": "4d488a78-54ab-4d13-8f9e-33289f8cfeec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I will to sweet some tread in Poin'd Volsces\n",
            "Of that he had rude the prison with you;\n",
            "In odns and the father's shoulders.\n",
            "\n",
            "First Musician:\n",
            "Soother, they if well\n",
            "sweet will not have no gracious bush, and fain and hear.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Thanks, heavy too for the son, a noble pardon,\n",
            "I would are succed each pity.\n",
            "\n",
            "PROSPERO:\n",
            "Late me the uncle Vould and his present a traitor,\n",
            "Bless me of his heir. Come, I have done\n",
            "For Clifford and The heavion of thine honest:\n",
            "I'll go asking your watch these people.\n",
            "\n",
            "First Murderer:\n",
            "Turn giddy, sir, as you cannot learn the tcoung,\n",
            "You shall go and bry weok to Romeo's are,\n",
            "That were my fancy to entreat of death.\n",
            "Tell them for what is dishand.'\n",
            "\n",
            "FLORIZEL:\n",
            "I speak not liberty and,\n",
            "That never made me stain'd the kinsman's wisdem,\n",
            "And we and to the boy of this sweeters\n",
            "Show me for, let them to see you, therefore I\n",
            "I know the cause, which have break me, wid,\n",
            "What entrance his contract that know\n",
            "I have in ware? Proshead, bedied the wold.\n",
            "\n",
            "JULIET:\n",
            "All formalainly  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.132620096206665\n"
          ]
        }
      ]
    }
  ]
}