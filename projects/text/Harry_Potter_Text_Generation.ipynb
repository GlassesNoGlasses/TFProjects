{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNd0Erbaq0RCVwKqSA6tIrn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GlassesNoGlasses/TFProjects/blob/main/projects/text/Harry_Potter_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone from GitHub repository\n",
        "\n",
        "!git clone https://github.com/GlassesNoGlasses/TFProjects.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu4jCbkHqwRN",
        "outputId": "a3540fc6-dea2-46fe-9bd1-26efc2c348a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TFProjects'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (14/14), 176.98 KiB | 4.21 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Generate text for a harry potter book. We will use RNN and Keras similar to the TensorFlow tutorial."
      ],
      "metadata": {
        "id": "QdvVW5H6oORm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Imports\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "xcOzsenRovGm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain harry potter books in .txt form\n",
        "\n",
        "#pathToFile = tf.keras.utils.get_file('harryPotterBook1.txt', 'file://content/TFProjects/data/texts/harryPotterBook1.txt')\n",
        "\n",
        "text = open('/content/TFProjects/data/texts/harryPotterBook1.txt', 'rb').read().decode(encoding='utf-8')"
      ],
      "metadata": {
        "id": "Fk3fWS3loxzi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in book 1\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XWY7HZ_rtT6",
        "outputId": "845dbb59-e4d8-4be6-e7b6-6fcaaffacd2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vocab into a list, then each character is tokenized with a unique id.\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "Adqs3Tmfs9AQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return characters based on their id representation defined above.\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "hf4bafz4t6HE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join ids back into original stirngs\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "N7QuoOkWuAkf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and assign character ids to all characters in original text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "\n",
        "# Convert ids into a stream of ids that represent the original text characters\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "R-WSGZCguH3G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence length of characters to train model on\n",
        "seq_length = 120"
      ],
      "metadata": {
        "id": "Aal1xYKGZ7FG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential batches of size seq_length + 1\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "wrPAkNK9Z_LH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are trying to predict the next character only."
      ],
      "metadata": {
        "id": "89eGoXPRaQC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split input sequence into a data set of (input, label)\n",
        "# I.e. \"tensorflow\" = (\"tensorflo\", \"ensorflow\")\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "a-8FGV5GaKR0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data set based on our original sequence\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "yNPw6MFXaZxe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test batches\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer to fit data into\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCmK6zSQajmg",
        "outputId": "23b3e604-89e2-41de-c35f-f636c42dff2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 120), dtype=tf.int64, name=None), TensorSpec(shape=(64, 120), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "NyDcOgwZaoZh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    # vocab_size: unique inputs + 1\n",
        "    # embedding_dim: output vector dimensions\n",
        "    # rnn_units: how many rnn used.\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    # log liklihood with vocab_size outputs\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "xjhjxepSa0HY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "GkSu9sfPa06l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "GWICr2oza37V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration of model with optimizer and loss functions\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "-qkLcyntbDXr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "gq-cq6TabIGL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual Training process\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogxhy44ObL_C",
        "outputId": "d6cf35b1-0a35-4bea-db44-1832ae6ba0df"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "56/56 [==============================] - 10s 71ms/step - loss: 3.3672\n",
            "Epoch 2/20\n",
            "56/56 [==============================] - 4s 64ms/step - loss: 2.4739\n",
            "Epoch 3/20\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 2.2376\n",
            "Epoch 4/20\n",
            "56/56 [==============================] - 5s 63ms/step - loss: 2.0590\n",
            "Epoch 5/20\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.8972\n",
            "Epoch 6/20\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.7597\n",
            "Epoch 7/20\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.6437\n",
            "Epoch 8/20\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.5452\n",
            "Epoch 9/20\n",
            "56/56 [==============================] - 5s 63ms/step - loss: 1.4651\n",
            "Epoch 10/20\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.3969\n",
            "Epoch 11/20\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 1.3394\n",
            "Epoch 12/20\n",
            "56/56 [==============================] - 5s 63ms/step - loss: 1.2854\n",
            "Epoch 13/20\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.2369\n",
            "Epoch 14/20\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.1905\n",
            "Epoch 15/20\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.1461\n",
            "Epoch 16/20\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.1034\n",
            "Epoch 17/20\n",
            "56/56 [==============================] - 4s 62ms/step - loss: 1.0576\n",
            "Epoch 18/20\n",
            "56/56 [==============================] - 4s 64ms/step - loss: 1.0137\n",
            "Epoch 19/20\n",
            "56/56 [==============================] - 4s 63ms/step - loss: 0.9684\n",
            "Epoch 20/20\n",
            "56/56 [==============================] - 5s 62ms/step - loss: 0.9196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Text Class\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "3P-5Un8Pb0V_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "gAmUVfKqb3k5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['CHAPTER'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JYXGJKMb4SG",
        "outputId": "269acd17-e48f-4419-949f-8c19a098332e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER FIRTT\n",
            "\n",
            "DIMAT OUREN\n",
            "\n",
            "Harry felt as though the nearest had wagan clitting moutly to the living room bardling he was a terrible snaps.\n",
            "\n",
            "\"Mysnock,\" said Hagrid under she let the trail in the\n",
            "edge.\"\n",
            "\n",
            "\"And what if Ron muttered,\" she said, his left exiced.\n",
            "\n",
            "\"See through the train,\" Hagrid whispered.\n",
            "\n",
            "Who looked as for it into the hoise, long, looking larged telling himself.\n",
            "\n",
            "\"Mars ignor?\"\n",
            "\n",
            "\"Why?\" he noise and laughed.\n",
            "\n",
            "\"Snape!\" Ron went off to\n",
            "his feet, and\n",
            "slithering voice an engule in castle -- they looked like a cholicat later.\n",
            "\n",
            "Sme liking Fluffy from holding behind gloats. Did next her neck,\n",
            "\"Snape's also covered -- friends!\" Hagrid pushed. \"For once he'd just looked as it broh. He\n",
            "pulled the des. Hagrid man his four heavy, grad their lead:\n",
            "\n",
            "And long black bitch of Hagrid's blank to asky from 16ville's Slitwle\n",
            "-- then morning, I mining warn you.\"\n",
            "\n",
            "They were all tres. He got back at the\n",
            "mad. Hagrid looked as though and thirteen that the porers didn't know\n",
            "what was going to prick to face a \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.66510009765625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "1or0fYcscRTs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orctigApcTUg",
        "outputId": "7e0b6ea3-fb70-4046-ef75-faf950dd77d4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56/56 [==============================] - 10s 65ms/step - loss: 3.3734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fecd872e500>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHE-PzTgcawQ",
        "outputId": "8b1f1327-94be-42c0-aeea-e2864f8a48ec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.8105\n",
            "Epoch 1 Batch 50 Loss 0.8192\n",
            "\n",
            "Epoch 1 Loss: 0.8189\n",
            "Time taken for 1 epoch 10.24 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 0.7337\n",
            "Epoch 2 Batch 50 Loss 0.8081\n",
            "\n",
            "Epoch 2 Loss: 0.7629\n",
            "Time taken for 1 epoch 5.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 0.6730\n",
            "Epoch 3 Batch 50 Loss 0.7236\n",
            "\n",
            "Epoch 3 Loss: 0.7059\n",
            "Time taken for 1 epoch 4.05 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 0.6321\n",
            "Epoch 4 Batch 50 Loss 0.6815\n",
            "\n",
            "Epoch 4 Loss: 0.6490\n",
            "Time taken for 1 epoch 4.09 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 0.5757\n",
            "Epoch 5 Batch 50 Loss 0.6177\n",
            "\n",
            "Epoch 5 Loss: 0.5898\n",
            "Time taken for 1 epoch 4.21 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 0.5267\n",
            "Epoch 6 Batch 50 Loss 0.5523\n",
            "\n",
            "Epoch 6 Loss: 0.5329\n",
            "Time taken for 1 epoch 4.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 0.4388\n",
            "Epoch 7 Batch 50 Loss 0.5176\n",
            "\n",
            "Epoch 7 Loss: 0.4781\n",
            "Time taken for 1 epoch 4.13 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 0.4068\n",
            "Epoch 8 Batch 50 Loss 0.4586\n",
            "\n",
            "Epoch 8 Loss: 0.4260\n",
            "Time taken for 1 epoch 4.08 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 0.3496\n",
            "Epoch 9 Batch 50 Loss 0.4013\n",
            "\n",
            "Epoch 9 Loss: 0.3782\n",
            "Time taken for 1 epoch 4.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 0.3143\n",
            "Epoch 10 Batch 50 Loss 0.3467\n",
            "\n",
            "Epoch 10 Loss: 0.3320\n",
            "Time taken for 1 epoch 5.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 11 Batch 0 Loss 0.2760\n",
            "Epoch 11 Batch 50 Loss 0.3143\n",
            "\n",
            "Epoch 11 Loss: 0.2917\n",
            "Time taken for 1 epoch 4.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 12 Batch 0 Loss 0.2385\n",
            "Epoch 12 Batch 50 Loss 0.2735\n",
            "\n",
            "Epoch 12 Loss: 0.2588\n",
            "Time taken for 1 epoch 4.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 13 Batch 0 Loss 0.2137\n",
            "Epoch 13 Batch 50 Loss 0.2491\n",
            "\n",
            "Epoch 13 Loss: 0.2298\n",
            "Time taken for 1 epoch 4.15 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 14 Batch 0 Loss 0.1954\n",
            "Epoch 14 Batch 50 Loss 0.2150\n",
            "\n",
            "Epoch 14 Loss: 0.2039\n",
            "Time taken for 1 epoch 4.64 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 15 Batch 0 Loss 0.1685\n",
            "Epoch 15 Batch 50 Loss 0.2020\n",
            "\n",
            "Epoch 15 Loss: 0.1806\n",
            "Time taken for 1 epoch 4.29 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 16 Batch 0 Loss 0.1488\n",
            "Epoch 16 Batch 50 Loss 0.1748\n",
            "\n",
            "Epoch 16 Loss: 0.1599\n",
            "Time taken for 1 epoch 4.23 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 17 Batch 0 Loss 0.1401\n",
            "Epoch 17 Batch 50 Loss 0.1532\n",
            "\n",
            "Epoch 17 Loss: 0.1448\n",
            "Time taken for 1 epoch 4.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 18 Batch 0 Loss 0.1179\n",
            "Epoch 18 Batch 50 Loss 0.1384\n",
            "\n",
            "Epoch 18 Loss: 0.1299\n",
            "Time taken for 1 epoch 4.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 19 Batch 0 Loss 0.1099\n",
            "Epoch 19 Batch 50 Loss 0.1250\n",
            "\n",
            "Epoch 19 Loss: 0.1191\n",
            "Time taken for 1 epoch 4.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 20 Batch 0 Loss 0.0984\n",
            "Epoch 20 Batch 50 Loss 0.1132\n",
            "\n",
            "Epoch 20 Loss: 0.1089\n",
            "Time taken for 1 epoch 4.53 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 21 Batch 0 Loss 0.0887\n",
            "Epoch 21 Batch 50 Loss 0.1074\n",
            "\n",
            "Epoch 21 Loss: 0.1004\n",
            "Time taken for 1 epoch 4.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 22 Batch 0 Loss 0.0864\n",
            "Epoch 22 Batch 50 Loss 0.0996\n",
            "\n",
            "Epoch 22 Loss: 0.0932\n",
            "Time taken for 1 epoch 4.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 23 Batch 0 Loss 0.0783\n",
            "Epoch 23 Batch 50 Loss 0.0905\n",
            "\n",
            "Epoch 23 Loss: 0.0871\n",
            "Time taken for 1 epoch 4.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 24 Batch 0 Loss 0.0780\n",
            "Epoch 24 Batch 50 Loss 0.0867\n",
            "\n",
            "Epoch 24 Loss: 0.0817\n",
            "Time taken for 1 epoch 4.13 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 25 Batch 0 Loss 0.0730\n",
            "Epoch 25 Batch 50 Loss 0.0812\n",
            "\n",
            "Epoch 25 Loss: 0.0774\n",
            "Time taken for 1 epoch 4.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 26 Batch 0 Loss 0.0718\n",
            "Epoch 26 Batch 50 Loss 0.0786\n",
            "\n",
            "Epoch 26 Loss: 0.0739\n",
            "Time taken for 1 epoch 4.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 27 Batch 0 Loss 0.0678\n",
            "Epoch 27 Batch 50 Loss 0.0761\n",
            "\n",
            "Epoch 27 Loss: 0.0709\n",
            "Time taken for 1 epoch 5.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 28 Batch 0 Loss 0.0657\n",
            "Epoch 28 Batch 50 Loss 0.0752\n",
            "\n",
            "Epoch 28 Loss: 0.0692\n",
            "Time taken for 1 epoch 4.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 29 Batch 0 Loss 0.0591\n",
            "Epoch 29 Batch 50 Loss 0.0744\n",
            "\n",
            "Epoch 29 Loss: 0.0676\n",
            "Time taken for 1 epoch 4.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 30 Batch 0 Loss 0.0623\n",
            "Epoch 30 Batch 50 Loss 0.0675\n",
            "\n",
            "Epoch 30 Loss: 0.0667\n",
            "Time taken for 1 epoch 4.21 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 31 Batch 0 Loss 0.0634\n",
            "Epoch 31 Batch 50 Loss 0.0753\n",
            "\n",
            "Epoch 31 Loss: 0.0664\n",
            "Time taken for 1 epoch 4.12 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 32 Batch 0 Loss 0.0598\n",
            "Epoch 32 Batch 50 Loss 0.0687\n",
            "\n",
            "Epoch 32 Loss: 0.0664\n",
            "Time taken for 1 epoch 4.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 33 Batch 0 Loss 0.0583\n",
            "Epoch 33 Batch 50 Loss 0.0703\n",
            "\n",
            "Epoch 33 Loss: 0.0669\n",
            "Time taken for 1 epoch 4.19 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 34 Batch 0 Loss 0.0590\n",
            "Epoch 34 Batch 50 Loss 0.0751\n",
            "\n",
            "Epoch 34 Loss: 0.0690\n",
            "Time taken for 1 epoch 4.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 35 Batch 0 Loss 0.0651\n",
            "Epoch 35 Batch 50 Loss 0.0929\n",
            "\n",
            "Epoch 35 Loss: 0.0782\n",
            "Time taken for 1 epoch 4.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 36 Batch 0 Loss 0.0828\n",
            "Epoch 36 Batch 50 Loss 0.4485\n",
            "\n",
            "Epoch 36 Loss: 0.2262\n",
            "Time taken for 1 epoch 4.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 37 Batch 0 Loss 0.4303\n",
            "Epoch 37 Batch 50 Loss 0.7604\n",
            "\n",
            "Epoch 37 Loss: 0.6670\n",
            "Time taken for 1 epoch 5.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 38 Batch 0 Loss 0.5372\n",
            "Epoch 38 Batch 50 Loss 0.5112\n",
            "\n",
            "Epoch 38 Loss: 0.5296\n",
            "Time taken for 1 epoch 4.16 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 39 Batch 0 Loss 0.3523\n",
            "Epoch 39 Batch 50 Loss 0.3571\n",
            "\n",
            "Epoch 39 Loss: 0.3497\n",
            "Time taken for 1 epoch 4.65 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 40 Batch 0 Loss 0.2378\n",
            "Epoch 40 Batch 50 Loss 0.2473\n",
            "\n",
            "Epoch 40 Loss: 0.2430\n",
            "Time taken for 1 epoch 4.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 41 Batch 0 Loss 0.1749\n",
            "Epoch 41 Batch 50 Loss 0.1803\n",
            "\n",
            "Epoch 41 Loss: 0.1769\n",
            "Time taken for 1 epoch 4.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 42 Batch 0 Loss 0.1256\n",
            "Epoch 42 Batch 50 Loss 0.1334\n",
            "\n",
            "Epoch 42 Loss: 0.1318\n",
            "Time taken for 1 epoch 4.20 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 43 Batch 0 Loss 0.0995\n",
            "Epoch 43 Batch 50 Loss 0.0992\n",
            "\n",
            "Epoch 43 Loss: 0.1015\n",
            "Time taken for 1 epoch 4.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 44 Batch 0 Loss 0.0794\n",
            "Epoch 44 Batch 50 Loss 0.0831\n",
            "\n",
            "Epoch 44 Loss: 0.0813\n",
            "Time taken for 1 epoch 5.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 45 Batch 0 Loss 0.0699\n",
            "Epoch 45 Batch 50 Loss 0.0726\n",
            "\n",
            "Epoch 45 Loss: 0.0694\n",
            "Time taken for 1 epoch 4.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 46 Batch 0 Loss 0.0583\n",
            "Epoch 46 Batch 50 Loss 0.0643\n",
            "\n",
            "Epoch 46 Loss: 0.0628\n",
            "Time taken for 1 epoch 5.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 47 Batch 0 Loss 0.0572\n",
            "Epoch 47 Batch 50 Loss 0.0622\n",
            "\n",
            "Epoch 47 Loss: 0.0599\n",
            "Time taken for 1 epoch 4.55 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 48 Batch 0 Loss 0.0535\n",
            "Epoch 48 Batch 50 Loss 0.0614\n",
            "\n",
            "Epoch 48 Loss: 0.0583\n",
            "Time taken for 1 epoch 4.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 49 Batch 0 Loss 0.0505\n",
            "Epoch 49 Batch 50 Loss 0.0594\n",
            "\n",
            "Epoch 49 Loss: 0.0576\n",
            "Time taken for 1 epoch 4.19 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 50 Batch 0 Loss 0.0499\n",
            "Epoch 50 Batch 50 Loss 0.0604\n",
            "\n",
            "Epoch 50 Loss: 0.0573\n",
            "Time taken for 1 epoch 4.27 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['CHAPTER 100'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states =  one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdvzYG4xczEP",
        "outputId": "5316a53c-e32f-4d91-b063-2f9038a8daf7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER 100\n",
            "\n",
            "\n",
            "INWARTS!\" he shouted.\n",
            "\n",
            "\"Let me see it!\" demanded Dudley.\n",
            "\n",
            "\"OUT!\" roared Uncle Vernon that people laughed; and saw that Hagrid had gotten used to this by now, but\n",
            "it had given him a bit of a shock on the first morning. He back down the\n",
            "corner of strangely dressed people and the next morning when they\n",
            "were all very impreces in the darkness.\n",
            "\n",
            "Charlie's friends were a cheery lot. They showed Harry and Hermione the\n",
            "harness they'd rate to do we to do. He shook his head and the\n",
            "books strange and except for his abyea, could go to Dumbledore.\n",
            "He hoped they were lost, warming toward the dungeon\n",
            "ceiling.\n",
            "\n",
            "\"I don't know,\" said Harry quietly. \"Ant me --\"\n",
            "\n",
            "A sudden noise outside in the corridor put an end to their discussion.\n",
            "They hadn't realized how much never belonged him and he caught a\n",
            "few moons when the compartment door slid open years.\n",
            "\n",
            "The baby banged its tail on the wall, making the windows rattle. Harry\n",
            "and Ron walked back to the castle for dinner, their pockets\n",
            "weighed down with the pas \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.8714439868927\n"
          ]
        }
      ]
    }
  ]
}