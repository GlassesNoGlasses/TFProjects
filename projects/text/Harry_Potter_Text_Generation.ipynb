{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMK+IF6pG/xL8QFTLRMUjGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GlassesNoGlasses/TFProjects/blob/main/projects/text/Harry_Potter_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone from GitHub repository\n",
        "\n",
        "!git clone https://github.com/GlassesNoGlasses/TFProjects.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu4jCbkHqwRN",
        "outputId": "a787fd69-1c79-41d8-ed67-89969fd80623"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TFProjects' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Generate text for a harry potter book. We will use RNN and Keras similar to the TensorFlow tutorial."
      ],
      "metadata": {
        "id": "QdvVW5H6oORm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Imports\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "xcOzsenRovGm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain harry potter books in .txt form\n",
        "\n",
        "#pathToFile = tf.keras.utils.get_file('harryPotterBook1.txt', 'file://content/TFProjects/data/texts/harryPotterBook1.txt')\n",
        "\n",
        "text = open('/content/TFProjects/data/texts/Harry_Potter_all_books_preprocessed.txt', 'rb').read().decode(encoding='utf-8')"
      ],
      "metadata": {
        "id": "Fk3fWS3loxzi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in book 1\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XWY7HZ_rtT6",
        "outputId": "742fff17-0862-4e9f-ef74-1f92f064335b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert vocab into a list, then each character is tokenized with a unique id.\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "Adqs3Tmfs9AQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return characters based on their id representation defined above.\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "hf4bafz4t6HE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join ids back into original stirngs\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "N7QuoOkWuAkf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and assign character ids to all characters in original text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "\n",
        "# Convert ids into a stream of ids that represent the original text characters\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "R-WSGZCguH3G"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence length of characters to train model on\n",
        "seq_length = 120"
      ],
      "metadata": {
        "id": "Aal1xYKGZ7FG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential batches of size seq_length + 1\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "wrPAkNK9Z_LH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are trying to predict the next character only."
      ],
      "metadata": {
        "id": "89eGoXPRaQC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split input sequence into a data set of (input, label)\n",
        "# I.e. \"tensorflow\" = (\"tensorflo\", \"ensorflow\")\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "a-8FGV5GaKR0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data set based on our original sequence\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "yNPw6MFXaZxe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating test batches\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer to fit data into\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCmK6zSQajmg",
        "outputId": "93283113-b33d-43b9-f7ee-5f8c5b55d78d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 120), dtype=tf.int64, name=None), TensorSpec(shape=(64, 120), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "NyDcOgwZaoZh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    # vocab_size: unique inputs + 1\n",
        "    # embedding_dim: output vector dimensions\n",
        "    # rnn_units: how many rnn used.\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    # log liklihood with vocab_size outputs\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "xjhjxepSa0HY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "GkSu9sfPa06l"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "GWICr2oza37V"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration of model with optimizer and loss functions\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "-qkLcyntbDXr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "gq-cq6TabIGL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual Training process\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogxhy44ObL_C",
        "outputId": "6e79cab7-9cf3-4964-a1a1-24ab20bcfe64"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "773/773 [==============================] - 57s 65ms/step - loss: 1.8153\n",
            "Epoch 2/20\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 1.2391\n",
            "Epoch 3/20\n",
            "773/773 [==============================] - 54s 67ms/step - loss: 1.1424\n",
            "Epoch 4/20\n",
            "773/773 [==============================] - 53s 65ms/step - loss: 1.0941\n",
            "Epoch 5/20\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 1.0607\n",
            "Epoch 6/20\n",
            "773/773 [==============================] - 54s 67ms/step - loss: 1.0348\n",
            "Epoch 7/20\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 1.0134\n",
            "Epoch 8/20\n",
            "773/773 [==============================] - 54s 67ms/step - loss: 0.9951\n",
            "Epoch 9/20\n",
            "773/773 [==============================] - 52s 65ms/step - loss: 0.9795\n",
            "Epoch 10/20\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 0.9662\n",
            "Epoch 11/20\n",
            "773/773 [==============================] - 54s 66ms/step - loss: 0.9552\n",
            "Epoch 12/20\n",
            "773/773 [==============================] - 54s 67ms/step - loss: 0.9465\n",
            "Epoch 13/20\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 0.9397\n",
            "Epoch 14/20\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 0.9339\n",
            "Epoch 15/20\n",
            "773/773 [==============================] - 54s 66ms/step - loss: 0.9309\n",
            "Epoch 16/20\n",
            "773/773 [==============================] - 53s 65ms/step - loss: 0.9285\n",
            "Epoch 17/20\n",
            "773/773 [==============================] - 55s 67ms/step - loss: 0.9281\n",
            "Epoch 18/20\n",
            "773/773 [==============================] - 52s 65ms/step - loss: 0.9286\n",
            "Epoch 19/20\n",
            "773/773 [==============================] - 55s 67ms/step - loss: 0.9296\n",
            "Epoch 20/20\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 0.9325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Text Class\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "3P-5Un8Pb0V_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "gAmUVfKqb3k5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['CHAPTER'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "8JYXGJKMb4SG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfdfe126-a130-4a83-b53a-30ffb853908a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER WI d HE DIDGS YOU TEOULL VIST TREAC !Ron and Hermione huddled for so long his body maiy .Two Monsters of Hogwarts crammed with a leap of memory on the nose of his face and showed that he was so he could still just stay on the house fists now in the dark ground .Are you not known know better this ?It it true however clear night concealed Malfoy and his voice pinning with to his legs again at each other into the air as the ghats rose into tears .Rabelook .well .Daylight for a wizard we came back again .And Amounted superviously bothered the wating wand still there was a slightly wash shaggy tinglegs of books on the floor let out a hand in frightly so that he broke the envelope .To Harrys old pieter .Didnt take this growl .So time you see said Harry What does that not it ?Im very proofill it it murder really lost .And when we brought this owls .Harry youve got to go to our first feagness .Be a real private hair of cheering the true had been very good to curse human life we got to answer  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2694084644317627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "1or0fYcscRTs"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "id": "orctigApcTUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38bae05e-f2a3-4d5b-a7a7-a017fcf77ee6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "773/773 [==============================] - 58s 67ms/step - loss: 1.7902\n",
            "Epoch 2/10\n",
            "773/773 [==============================] - 53s 65ms/step - loss: 1.2307\n",
            "Epoch 3/10\n",
            "773/773 [==============================] - 53s 66ms/step - loss: 1.1376\n",
            "Epoch 4/10\n",
            "773/773 [==============================] - 52s 65ms/step - loss: 1.0908\n",
            "Epoch 5/10\n",
            "773/773 [==============================] - 54s 66ms/step - loss: 1.0577\n",
            "Epoch 6/10\n",
            "773/773 [==============================] - 52s 65ms/step - loss: 1.0323\n",
            "Epoch 7/10\n",
            "773/773 [==============================] - 52s 65ms/step - loss: 1.0111\n",
            "Epoch 8/10\n",
            "773/773 [==============================] - 52s 65ms/step - loss: 0.9931\n",
            "Epoch 9/10\n",
            "773/773 [==============================] - 53s 65ms/step - loss: 0.9778\n",
            "Epoch 10/10\n",
            "773/773 [==============================] - 52s 65ms/step - loss: 0.9652\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f747c751870>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "id": "YHE-PzTgcawQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "565470c4-b066-4f2b-9c16-142b92b062b0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.9822\n",
            "Epoch 1 Batch 50 Loss 0.9678\n",
            "Epoch 1 Batch 100 Loss 0.9402\n",
            "Epoch 1 Batch 150 Loss 0.9641\n",
            "Epoch 1 Batch 200 Loss 0.9406\n",
            "Epoch 1 Batch 250 Loss 0.9335\n",
            "Epoch 1 Batch 300 Loss 0.9522\n",
            "Epoch 1 Batch 350 Loss 0.9485\n",
            "Epoch 1 Batch 400 Loss 0.9782\n",
            "Epoch 1 Batch 450 Loss 0.9511\n",
            "Epoch 1 Batch 500 Loss 0.9701\n",
            "Epoch 1 Batch 550 Loss 0.9864\n",
            "Epoch 1 Batch 600 Loss 0.9507\n",
            "Epoch 1 Batch 650 Loss 0.9751\n",
            "Epoch 1 Batch 700 Loss 0.9670\n",
            "Epoch 1 Batch 750 Loss 0.9448\n",
            "\n",
            "Epoch 1 Loss: 0.9545\n",
            "Time taken for 1 epoch 53.83 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 0.9777\n",
            "Epoch 2 Batch 50 Loss 0.9314\n",
            "Epoch 2 Batch 100 Loss 0.9465\n",
            "Epoch 2 Batch 150 Loss 0.9761\n",
            "Epoch 2 Batch 200 Loss 0.9513\n",
            "Epoch 2 Batch 250 Loss 0.9450\n",
            "Epoch 2 Batch 300 Loss 0.9426\n",
            "Epoch 2 Batch 350 Loss 0.9313\n",
            "Epoch 2 Batch 400 Loss 0.9598\n",
            "Epoch 2 Batch 450 Loss 0.9211\n",
            "Epoch 2 Batch 500 Loss 0.9366\n",
            "Epoch 2 Batch 550 Loss 0.9274\n",
            "Epoch 2 Batch 600 Loss 0.9573\n",
            "Epoch 2 Batch 650 Loss 0.9564\n",
            "Epoch 2 Batch 700 Loss 0.9461\n",
            "Epoch 2 Batch 750 Loss 0.9696\n",
            "\n",
            "Epoch 2 Loss: 0.9463\n",
            "Time taken for 1 epoch 50.45 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 0.9792\n",
            "Epoch 3 Batch 50 Loss 0.9651\n",
            "Epoch 3 Batch 100 Loss 0.9524\n",
            "Epoch 3 Batch 150 Loss 0.9447\n",
            "Epoch 3 Batch 200 Loss 0.9111\n",
            "Epoch 3 Batch 250 Loss 0.9024\n",
            "Epoch 3 Batch 300 Loss 0.9266\n",
            "Epoch 3 Batch 350 Loss 0.9481\n",
            "Epoch 3 Batch 400 Loss 0.9268\n",
            "Epoch 3 Batch 450 Loss 0.8915\n",
            "Epoch 3 Batch 500 Loss 0.9069\n",
            "Epoch 3 Batch 550 Loss 0.9186\n",
            "Epoch 3 Batch 600 Loss 0.9375\n",
            "Epoch 3 Batch 650 Loss 0.9660\n",
            "Epoch 3 Batch 700 Loss 0.9488\n",
            "Epoch 3 Batch 750 Loss 0.9622\n",
            "\n",
            "Epoch 3 Loss: 0.9396\n",
            "Time taken for 1 epoch 51.63 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 0.9530\n",
            "Epoch 4 Batch 50 Loss 0.9320\n",
            "Epoch 4 Batch 100 Loss 0.9532\n",
            "Epoch 4 Batch 150 Loss 0.9165\n",
            "Epoch 4 Batch 200 Loss 0.9344\n",
            "Epoch 4 Batch 250 Loss 0.9086\n",
            "Epoch 4 Batch 300 Loss 0.9218\n",
            "Epoch 4 Batch 350 Loss 0.9091\n",
            "Epoch 4 Batch 400 Loss 0.9419\n",
            "Epoch 4 Batch 450 Loss 0.9471\n",
            "Epoch 4 Batch 500 Loss 0.9630\n",
            "Epoch 4 Batch 550 Loss 0.9241\n",
            "Epoch 4 Batch 600 Loss 0.9263\n",
            "Epoch 4 Batch 650 Loss 0.9401\n",
            "Epoch 4 Batch 700 Loss 0.9452\n",
            "Epoch 4 Batch 750 Loss 0.9473\n",
            "\n",
            "Epoch 4 Loss: 0.9351\n",
            "Time taken for 1 epoch 52.07 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 0.9479\n",
            "Epoch 5 Batch 50 Loss 0.9340\n",
            "Epoch 5 Batch 100 Loss 0.8956\n",
            "Epoch 5 Batch 150 Loss 0.9245\n",
            "Epoch 5 Batch 200 Loss 0.9299\n",
            "Epoch 5 Batch 250 Loss 0.9267\n",
            "Epoch 5 Batch 300 Loss 0.9297\n",
            "Epoch 5 Batch 350 Loss 0.9118\n",
            "Epoch 5 Batch 400 Loss 0.9389\n",
            "Epoch 5 Batch 450 Loss 0.9305\n",
            "Epoch 5 Batch 500 Loss 0.9472\n",
            "Epoch 5 Batch 550 Loss 0.9393\n",
            "Epoch 5 Batch 600 Loss 0.9312\n",
            "Epoch 5 Batch 650 Loss 0.9380\n",
            "Epoch 5 Batch 700 Loss 0.9430\n",
            "Epoch 5 Batch 750 Loss 0.9329\n",
            "\n",
            "Epoch 5 Loss: 0.9319\n",
            "Time taken for 1 epoch 51.66 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 0.9682\n",
            "Epoch 6 Batch 50 Loss 0.9070\n",
            "Epoch 6 Batch 100 Loss 0.9288\n",
            "Epoch 6 Batch 150 Loss 0.9072\n",
            "Epoch 6 Batch 200 Loss 0.9208\n",
            "Epoch 6 Batch 250 Loss 0.9017\n",
            "Epoch 6 Batch 300 Loss 0.9599\n",
            "Epoch 6 Batch 350 Loss 0.9294\n",
            "Epoch 6 Batch 400 Loss 0.9262\n",
            "Epoch 6 Batch 450 Loss 0.9162\n",
            "Epoch 6 Batch 500 Loss 0.9419\n",
            "Epoch 6 Batch 550 Loss 0.9490\n",
            "Epoch 6 Batch 600 Loss 0.9684\n",
            "Epoch 6 Batch 650 Loss 0.9284\n",
            "Epoch 6 Batch 700 Loss 0.9328\n",
            "Epoch 6 Batch 750 Loss 0.9421\n",
            "\n",
            "Epoch 6 Loss: 0.9300\n",
            "Time taken for 1 epoch 51.71 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 0.9558\n",
            "Epoch 7 Batch 50 Loss 0.9414\n",
            "Epoch 7 Batch 100 Loss 0.9258\n",
            "Epoch 7 Batch 150 Loss 0.9401\n",
            "Epoch 7 Batch 200 Loss 0.9373\n",
            "Epoch 7 Batch 250 Loss 0.9088\n",
            "Epoch 7 Batch 300 Loss 0.9286\n",
            "Epoch 7 Batch 350 Loss 0.9061\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-db2ffa3c82d7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_n\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mobj_update_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 )\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, values, sample_weight)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTED_MEAN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                 \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mnum_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         )\n\u001b[1;32m   1023\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   2054\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2057\u001b[0m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[1;32m   2058\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Harry Potter was dead.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states =  one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "zdvzYG4xczEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f15ca35-4238-458c-92a1-dd5e5934869c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry Potter was dead.Skells did everybody echoed Harry added behind them so that he could not be remonstrating to sound as though if I were all yelling in the dark end students will take plenfylusion of Sirius Black had heard .The whole farther or out for us all three of us is only said Harry quietly .Please meet these things along in the class and Cedric .Dad will corridor Miss Nor much Sirius looked down at his own last sparks .Slowly very still longly very protected and happened to help him who Still said the Malfoy was already shrugging .When he lay moving out of the way and entered .Oh ever he just unseen pures !said Bagman pounding his hand at Cedrics .Where ?Sorry hes alrowed dreaming that the curse was unhalk to him why dont you make everyone dose that the Death Eaters ahead these words will matter of what he had said about anything else ?said Harry quickly shifted .My parents !said Mr Weasley quietly .Seither than depressed it practical echoes about okay Tom sick .He took a deep breakhing whirled  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.935009717941284\n"
          ]
        }
      ]
    }
  ]
}